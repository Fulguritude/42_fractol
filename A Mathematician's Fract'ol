I personally love mathematics. And yet, I used to hate them in school. Hate them. I had straight 20/20s, but I *still* hated them. Then at some point I started reading mathematics on my own, and I discovered that math class in school (from 1st grade to your Master's) was solfege without music. I discovered that some well-meaning idiot, somewhere, decided that if I couldn't read a partition, I was too dumb to listen to music, too dumb to appreciate Bach or Eminem, too dumb to start plucking strings on a guitar until I got a bit of it right. https://www.maa.org/external_archive/devlin/LockhartsLament.pdf 

I discovered what mathematics actually were.

If I had to describe what they are to me today, I would have to describe it as a "bestiary" of strange structures that have a certain "behavior", and "relationships" to one another. I almost mean that in an animal or even a human sense, perhaps because I've come to see all entities and the ecosystems in which they evolve more and more as special kinds of mathematical systems. Additionally, this bestiary is at the same time the best way I have found to make sense of most of the new ideas I will ever receive or have - a very, very powerful way of deconstructing the complexity of our world. I also discovered that mathematics were nothing like what I had been taught, and were hidden everywhere I looked. Indeed, I consider that I have learned more in my life about mathematics and research (and how they relate to the world around us) by playing Zelda, than I ever did in school.

I fell in love. A form of spiritual love, I guess. Mathematics, today, is my religion, if you could call it that. Let's try to give you an idea of my point of view with an analogy. Let's say there IS a creator to this world. If you look at it closely enough, this world is a world of mathematics at every scale and every turn. Look at our world-building: no other work in the history of mankind has come close to video games in recreating fictional physical systems; in building universes themselves. Video games use code, the 'instrument' of mathematics, to invent a simulacrum of weird spacetime and matter and complex objects. To design and code a video game is literally to "play god". From these observations it follows that our supposed creator is also a mathematician. And I will honor this god's work better by learning the language in which this god speaks -the *Word* as *Mathematics*-, than I ever will by reading millenium-old books polluted by the imprecision of human languages and human historical biases.

Even though I am an atheist, I have found more beauty and existential answers in mathematics than I have anywhere else, and boy have I looked a lot of other places. I find religions and theology very interesting: there is some structure, logic, philosophy and epistemology to them, when they are done well, and they are interesting to explore. But there are always too many looming mistakes and weirdness or madness in them. I wouldn't judge religious books so severely had I not read most of them at least a decent amount and could defend my position. Yet, no religion that claims to be "revealed" has actually been equal in my eyes to the actual feeling of "reveal" you get when you realize the structure you thought you built, you thought you invented, had already constructed its own behavior, seemingly independently from you, like it always existed outside your own mind and you're just "discovering" it, not "inventing" it... Every theorem is both a feat of engineering and a natural observation of the cosmos and its laws.

This is a sentiment shared by almost all professional mathematicians, and reveals just how deep mathematics really go in understanding ourselves and our place in the universe.

So this text is a love letter to the "music" and "spirituality" that is mathematics, one of the many such love letters I will write in my lifetime. It seeks to show the kind of mathematics that is hidden from most people by our school systems. It is an essay that should allow anyone doing the Fract'ol project to learn more about why mathematics is truly music, and not just solfege.


**I) What is a fractal ?**

A key point in mathematics, and for the human mind to make sense of the world, is to define things. Defining is the art of decoding what something *is*, and what something *isn't*. To be done well, this requires at least two parts: formal definition, and intuitive definition.

Formal definitions are the kind you'd find in a dictionary: they describe what a thing is, and what it isn't. They draw limits around concepts. Is China a democracy, is India, is France, is the United States ? You'd have to have a very intelligent, and very precise way of describing what a democracy is to answer this question. If your definition is flimsy, imprecise, self-contradictory: all that you will deduce from it is *worthless*. Even then, definitions are still arbitrary, axiomatic. There is no way to "prove mathematically" that some definition of democracy is better than another one. In philosophy, one uses "stipulative definitions", one says "well, for the rest of the essay, we'll be using THIS definition of democracy to make sure our analysis stays coherent, at least with itself".

But in the world of mathematics, which is much purer, simpler, less prone to noise and uncertainty, which is in a sense "generated by the definitions we choose", **formal definitions are the core of abstraction. They are what allow us to simplify extremely complex behavior into a very simple set of rules or ideas.** 

So let's define something complex formally to give an example.

For example, a "vector space" (oooooh, scary words), when you take the core of the formalism and explain it, is just a certain way of using addition within a single structure (called an "abelian group", a structure which works like the unsigned integers using only addition). BUT, you also add a certain way of multiplying (called scaling) the elements of the first structure with elements from another structure (called a "field", which works like the real or the complex numbers with addition, subtraction, multiplication and division not by 0). In this context, the elements of the first structure, the abelian group, are called vectors - and the elements of the second structure, the field, are called scalars. When you put these two structures together in the right way with these operators of "add" and "scale", you get what is called a "vector space". Polynomials, mathematical functions and tuples are all examples of 'vectors', when defined in this way. 

But how does this work ? I mean, that's just a lot of big words and hoopla, no ?

Practically, what this "formal definition" actually is, is a series of 8 or 9 rules to follow for what you are allowed to do with vectors and scalars [if you want the rules: https://en.wikipedia.org/wiki/Vector_space#Definition ], and just from these few rules you can build and deduce extremely complex things, extremely rich and complex behavior for vector spaces. And this exploration allows you to understand how you transform structures and the information they contain from one to another. You get an instinct for the overall map of mathematics.

For example, every matrix is a transformation of a vector space into another vector space (or just a different version of itself for square matrices), and every vector is a n×1 matrix, so can be considered itself to be a transformation from a vector space to another, but you wouldn't realize that at first. Especially considering the fact that "tuples aren't just tuples": n×1 (vectors) and 1×n ('covectors', 'linear forms') tuples can relate to each other differently within the right context.

But even though formal definitions act as an important "abstract hub" in your mind, a hub through which you can analyze with common tools cases that could have looked different at first, this isn't enough for our imperfect human minds. We cannot build abstraction without a mass of examples, as the following image shows.

https://imagesvc.timeincapp.com/v3/mm/image?url=https%3A%2F%2Ffortunedotcom.files.wordpress.com%2F2016%2F08%2Fscreen-shot-2016-08-09-at-11-17-46-am.png&w=1100&q=85

Quite evidently, understanding the abstract concept of "the letter A" is a lot easier with contrasting cases. This is how learning works everywhere. You NEVER learn better by learning less, by sticking to the simple things and doing them over and over again. You learn better through contrast, difference, interconnection: distinction and commonalities. Even our AIs are susceptible to "overfitting" (learning without being able to generalize) if they don't learn from contrasting cases ! So it's not even limited to human or animal intelligence, but any computational intelligence that relies on a trial-error-correction algorithm, like darwinism or deep learning, to function. This brings us to the second part of the art of defining: intuitive definition.

Intuitive definition is even more important, because **the price of abstraction is erudition**. Intuitive definition consists mostly of well-constructed examples which illustrate the definition and its boundaries. It is hard to understand why a vector space is such an important and useful concept when presented abstractly if you haven't seen lots of examples of vector spaces, and seen that they are EVERYWHERE. The most basic example you know are R² and R³: R² is the 2D cartesian plane (the universe in which exists a frame of Super Mario Bros, if you could go up, down, right, left infinitely), R³ is the 3D cartesian cartesian plane (the universe in which a frame of Call of Duty exists, if you could go up, down, right, left, forward, back infinitely). You could consider our own spacetime to be a version of R⁴ (3 spatial dimensions, and 1 time dimension), though this is up for debate (you can't really go back in time apparently; also, most of quantum mechanics relies on complex numbers, and complex vector spaces have double the dimension of real vector spaces if you try to convert them into real vector spaces). [Solve this problem for a Nobel prize, fame, maybe fortune, and certainly your name in the history books !]

But there are many other examples of vector spaces, for example function spaces. Take f(x) := x², a mathematical function with a single input x, and which returns (x*x). Or g(x) := 2x, a mathematical function which takes in a signle input x, and returns (2*x). Say the input space is R (x is real number, ie, a float with infinite precision) and the return space is R (x*x and 2*x are still reals/floats) for both of these functions. This means that in this context, f and g exist in the same space, called (R -> R), of functions from R to R. And you can add them just like you would vectors, with the same behavior as when you add two cartesian vectors in R² or R³ together. If we take two cartesian vectors u=(2, 0, 5) and v=(0, -3, 1), then u+v = (2, -3, 6). Similarly, f + g = h, where h(x) := x² + 2x. You can also scale functions. If u is the tuple (5, -2, 3) then 3u = (15, -6, 9). Equivalently, (3h)(x) := 3x² + 6x. You can also subtract, because you can scale by -1. For example, we have h - g = f if we keep the same example. Etc.


Now, to fractals.

Formally, there are many different types of mathematical objects that can be described by the word "fractal". When there is ambiguity like this, it is very important to define which type of fractal you are referring to. For this reason, we'll move on to the intuitive definition. 

[This page can give you a roadmap for a more profound understanding of fractals if you'd like, https://www.mi.sanu.ac.rs/vismath/javier1/index.html . Dynamical systems are still an area of research both in math and CS, because they're one of the best tool we currently have to analyze and predict chaos, and chaos hasn't been solved. The idea of the "butterfly effect" (that an infinitesimal variation can, with enough time, lead to two completely different behaviors for the state of the system studied) comes from dynamical systems, if you've ever heard of it. A lot of biological mathematics are deeply rooted in dynamical systems (your veins are sort of fractal; so is the generation of structures in many plants, like roots or leaves).]

Intuitively, fractals are the most famous and visually striking example (with cellular automata) of one of the most important idea to understand our universe and how it works: the idea that **from very simple rules can emerge extremely complex behavior** within a system. The idea that creationists have the most trouble with, and yet is completely, undeniably true ! You don't need a creator: you need simple rules that lead to complexity.

Most famous fractals are self-similar (they repeat within themselves), but this is not essential. Generally, also, their dimension is not an integer. This means you can't really "measure" a fractal. Why ? Well, when you're looking at something in dimension 1, like a distance between two points, you take the "length" between them (meters). If you're measuring something 2D, you're using "area" (m²). Something 3D, "volume" (m³). And for everything above 3, for every "hypervolume", you use the 'Lebesgue measure of dimension n' (this means that "length" is the "Lebesgue measure of dimension 1", that "area" is the "Lebesgue measure of dimension 2", etc). But when your structure has a non-integer dimension, some very weird things start to happen if you try to measure it with the usual mathematical tools. Take the Koch snowflake ( https://en.wikipedia.org/wiki/Koch_snowflake ), it has infinite perimeter ("length" of boundary) but finite "area"... To answer this, mathematicians came up with various ways of analyzing the dimension of fractals, most famous of which is Haussdorff's definition for fractal dimension, which generally gives a real number as a result, not just an integer.

To truly understand fractals, one should understand the notion of a "continuous but nowhere differentiable function", which, if you haven't done at least high school math, would be a bit difficult to explain within the scope of this text (but if you're interested, 3blue1brown's "Essence of Calculus" series is a good place to start). There's a whole field that studies the continuity and variations of functions, called topology, which is still to this day one of the biggest branches of modern mathematics. Everything from fluid flow to statistics depends on topology to some extent. If you want to understand "continuous but nowhere differentiable", think of a curve that never disconnects, or a surface that is nice and smooth with no abrupt vertical cliffs; but that changes so drastically in height (both going up or down) so fast or weirdly at every step in every direction that you couldn't predict if you'd end up or down from where you were before after taking a step. Yeah, that's weird. Yet look at the boundary of the Mandelbrot set, zoom in on it, and you'll get a clear sense of what that means.

So that's why **intuitively, fractals should just be understood as a kind of general term for weird representations of chaotic, self-similar, or infinite systems.**

I personally think it'd be a good avenue for mathematics research to study "the space in which all fractals of a certain type exist", and to improve on this categorization. But to my knowledge, this hasn't been done yet. Anyone here aiming for the Fields medal ?


**II) Complex numbers and polynomials**

The complex numbers (and spaces of polynomials) form what we call in mathematics an "algebra". This is not to be confused with "algebra" meaning the "branch of mathematics that uses symbolic systems and methods to reason about things our human minds cannot visualize", ie, the largest branch of mathematics along with geometry and theoretical computer science (algebra as the part of mathematics most people think about when they use the word "mathematics"). Used in this context, an 'algebra' is a vector space to which we have given a third operator which is a new form of multiplication between vectors.

Let's recap: an algebra is a vector space with a third operator. The first operator is vector addition (input two vectors, returns a vector), the second operator is vector scaling (input a scalar and a vector, return a vector: the input isn't exactly the same as vector addition), the third operator is a form of multiplication that takes two vectors as input and returns a vector as output (vector×vector->vector, same function signature as vector addition). **When a vector space has such a multiplication, we call it an algebra.** If you take our previous example of (R -> R) with f and g, you could define "f × g = h" where h(x) := (x²)(2x) = 2x³, for example, and that would be an example of the type of (vector×vector -> vector) multiplication you find in an algebra.

[Note that the scalar product/dot product of two vectors in euclidean geometry is different because it is vector×vector -> scalar. The cross product of R³ DOES make R³ into an algebra though, though its properties are weird. Also note that not every algebra has the same properties, because third operators are diverse. Some associative, some aren't; some have a neutral element (like +0 or *1 or +"" which don't change anything), some don't; some are commutative, some are anticommutative, some are neither, etc. For now, we'll stick to concrete examples.]

We refer to the space of complex numbers as 'C'. In the case of complex numbers, you start out with R², the vector space of 2-tuples over the real numbers. Each 2-tuple can be written (a, b) = (a, 0) + (0, b) = a(1, 0) + b(0, 1) = a×1 + b×i = a + ib, where 1 is the point (1, 0), i is the point (0, 1). Notice how we used the two vector space operators (add and scale) to go from one form to the other very easily; and how this illustrates the obvious links between C and R².

But now we want to make it an algebra, so we're gonna add a form of vector×vector->vector multiplication. The multiplication we use in this space C works as follows: (a, b) × (c, d) = (ac - bd, ad + bc). In this system, i² = (0,1)² = (0,1)×(0,1) = (0×0 - 1×1, 0×1 + 1×0) = (-1, 0) = -1. And thus, i is one of the two square roots of -1 (-i is the other square root of -1, try it). [Also, the reason why every complex number has 2 square roots is very important, and will be discussed below.]

All of this is rather helpful because you can think of addition in the complex plane as vector addition in R². As for multiplication, the intuitive way to understand it would require you to understand what we call the 'polar form' of complex numbers, namely r*e^(i*theta). In this form, r stands for "radius", also called "modulus" or "absolute value" or "norm" of a complex number: it is the length of your 2D vector. Theta, called the "argument" of a complex number, is the angle that that vector makes with the vector (1, 0). [Note that an angle between vectors is always considered with the point (0, 0) as the summit of the angle.] This means the number i has an argument of 90 degrees, or better expressed, pi/2 radians (or tau/4 if you're smart and use tau = 2 * pi). You loop around the plane to come back to these positive real numbers if you have an argument that correspond to an angle of 2*pi radians (~6.28), which means 360°, or an integer multiple of that (k*2pi where k is an int). It's like a clock but with 2pi minutes, if that non-integer amount of time makes any sense to you (more info: "modular arithmetic" and/or "periodicity"). Check out stuff on Euler's formula and Euler's identity for more info on the polar form and its relation to the vector form.

In this polar form, multiplication of complex numbers is easy: (r1 × e^(i × t1)) × (r2 × e^(i × t2)) = (r1×r2) × e^(i×(t1 + t2)). You just multiply the radii on one side, add the arguments on another, and voilà, that's complex multiplication intuitively. Geometrically, your point gets closer to the point (0, 0) if it is multiplied by a complex number inside the unit circle; and gets further away from (0, 0) if multiplied by a number outside this circle. Also, the angle of the resulting vector with the vector (1, 0) is obviously the sum of the angles that the summand vectors respectively have with (1, 0) (like turning a clock 5 hours then 3 hours turns a clock 8 hours, and the angle made by the hands of the clock sum nicely as as well; with the distinction that adding a positive angle is a counterclockwise rotation in the complex plane, rather than clockwise like a positive rotation on a clock).


Remember when I said every complex number had two square roots ? Well it also has 3 cubic roots, and 4 quartic roots, etc. But to understand this, we must look at polynomials.

The space of all polynomials constitute another algebra. Like all vector spaces, you can base them on any 'field', any structure that follows the same rules as R or C, but we'll stick with polynomials over C in our case. Let's say 'z = a + ib' is a complex number. A polynomial over C is an expression of the form "a0 + a1×z + a2×z² + a3×z³ + ...", potentially infinitely. The a_n numbers are called the coefficients of the polynomial. Each a_n×z^n part of the sum is called a monomial. z is called the indeterminate, or the variable, of your polynomial. Each polynomial can be uniquely described as a vector of its coefficients. For example, you could write the polynomial "4z⁷ + 5z² + 2z + 1" as the vector (1, 2, 5, 0, 0, 0, 0, 4, 0, 0, 0, ...). Note that this vector exists in a vector space of *infinite* dimension. Adding and scaling polynomials is like adding and scaling vectors. As for the multiplication of polynomials, it's the most intuitive you could imagine. I'll give you an example rather than a formula, for everything else there's WolframAlpha : (3z² + 2) × (4z³ + z) = 12z⁵ + 11z³ + 2z. This type of multiplication is called a convolution and is one of the classical problems of high-performance computing (google Fast Fourier Transform).
'Evaluating' a polynomial is calculating its value when you specify the value of z in input. For example, if z = i, then z² + 2z + 1 returns/evaluates to 2i. The highest exponent for z is called the degree of the polynomial (for example, 12z⁵ + 11z³ + 2z is of degree 5; and when you multiply two polynomials, their degrees are summed to find the degree of the result).

Polynomials play a fundamental role in mathematics. The Stone-Weierstrass theorem uses polynomials to translate the language of cartesian vector spaces into the language of functional vector spaces (see https://en.wikipedia.org/wiki/Taylor_series ) and is fundamental in finding fast approximation for mathematical functions iteratively (the standard <math.h> uses this trick abundantly, how else can you find ln(x) and exp(x) for an arbitrary x fast enough ?). Another point of relevance is that polynomials of degree n over the complex plane ALWAYS have EXACTLY n points of the complex planes for which that polynomial returns 0 when it is evaluated. These points that return 0 are called the "roots" of the polynomial, and this fact is so important it is called "the fundamental theorem of algebra", with good reason. You might remember finding roots for polynomials of degree 2 in high school (remember 'b² - 4ac' and its friends ? You'll see it again in ray-tracing).

Finally, what the Fundamental Theorem of Algebra teaches us is that all polynomials can be written in either an additive form (the kind we've used until now, a sum of monomials), or a multiplicative form, using the roots. I'll give you just one example: 2z² + 2 = 2(z² + 1) = 2(z - i)(z + i), where i and -i each cause one of the terms in the parentheses to return 0, and are thus the roots of our polynomial.

Now that you've seen some examples, don't you agree it was useful to be able to synthesize something as complex as the complex numbers or the polynomials to "you can add, scale, and multiply; the rest is fluff" ? And when you'll see the term vector space for some weird new structure, you'll know "oh, I can add and scale like I would normally do, that's cool".
Also note that we've only seen examples of commutative algebras here (ie, where you always have f×g == g×f). But spaces of matrices (like the space of all matrices of size 3×4) for example are non-commutative algebras (you generally have A×B != B×A for two matrices A and B).

[Also, if you want a real mindf*#$, know that you can make fractions of polynomials (like "P = (z² + 3z + 4) / (z³ + 2)" for example). It JUST SO HAPPENS that the space of fractions of polynomials follows the rules of a 'field' like R and C (you can do addition and multiplication like we're used to, and can subtract and divide everything without a problem except divide by 0). So TECHNICALLY, you can build a vector space where the --scalars-- are the --polynomial fractions--... like another vector space of polynomials, for example, but where polynomial fractions in a different variable are scalars ! There are many more crazy things you can come up with once you start getting to know the bestiary of algebraic structures and their relationships.]

You also have polynomials in multiple variables, like 3x² + 2x²y + 2y + 3y³. The field which studies them is called algebraic geometry, and is apparently one of the most complex and beautiful branches of mathematics to study (it is said the field was kind of killed by Grothendieck who sort of figured out everything already, and I haven't had the time to get into it myself).

R[X] is the space of polynomials in one variable over R, C[X] over C; R[XY] is the space of polynomials in two variables over R, C[XY] over C, etc. R(X) is the space of polynomial fractions in one variable over R, and you also have R(XY), C(X), C(XY), etc. 
[The weird space of polynomial fractions we described above using C(X) as its basic field could be referred to as C(C(X)), and if I recall, it is isomorphic (~equivalent) to C(XY), but don't quote me on that. If it is indeed, that could serve as an excellent example of the usefulness of the isomorphism as a concept for someone who doesn't know about it; as C(XY) is obviously a better way of framing the problem than C(C(x)).]


**III) Escape-time fractals**

Given that Fract'ol requires us to do both Mandelbrot and Julia, the goal I set myself was to make a software that could render *any* escape-time fractal (ETF) - at least within reasonable computational limits. ETFs form the category of fractals in which both Mandelbrot and Julia sets are included.

Why did I set this objective ? Part of it for the challenge, mostly because I wanted my computer to generate crazy, unexpected, beautiful stuff.

An escape-time fractal can basically be formally summarized into two steps: one, take a polynomial; two, use a certain protocol which will apply that polynomial repeatedly over points of the complex plane (eventually with some tweaks here and there). Every starting point/complex number's radius will either stay close to zero, and blow up towards infinity. For this reason, you can then categorize each point of the plane according to how long they take to escape the region centered around (0, 0) in the complex plane; or just say they never escaped. We generally consider they've escaped once the radius of the complex number becomes above 2 (because it'll keep getting bigger with even just a z² in the polynomial formula; and in reality changing this number does not change the fractal, so long at it doesn't get too close to 1., or under 1.). We call this protocol the "dwell function", because it tells you how long each point you're studying will "dwell" (stick around) within the region of the plane where we're still unsure if it goes towards zero or towards infinity. Once a number is still close to (0, 0) even after a MAX_DWELL number of iterations, we consider it to not have escaped.


There is some nuance here with the Newton dwell function, which checks convergence (ie, how close your number gets) to the roots of the polynomial rather than behavior of the radius of the complex number thats being iterated upon.

There are very probably dwell functions that no one has thought of before, and that generate fractals that no one knows anything about, but could have real artistic or mathematical significance (I invented one on my own as a proof of concept).


**IV) Implementation**

The major thing I did for implementation was making dwell functions which could call an abstract polynomial and evaluate it, and have these dwell functions all share the same signature. This allowed me to use them as a pointer to a function within my t_fractol.

What I did is that the first argument to my program (say 'julia') sets the chosen (julia) dwell function, and some default settings (notably the default polynomial called). The second argument, which is optional, is a filepath to a file which contains a single line, which is a list of floats (I coded an ft_atolf/ft_atod to read floats for this purpose; if you also want to see my post for float printing, which delves into arbitrary precision arithmetic, you can find it here: https://forum.intra.42.fr/topics/18827/messages ). This list of floats generates the polynomial or polynomial fraction that will be called by the dwell function. This allows the user to call the Mandelbrot dwell function over the polynomial "-3.5z⁷ + 3z³ + c", for example. Come on, you *must* be curious as to what happens when you try something random like that.

I implemented the operators on complex numbers in a rather straightforward manner. I might make them accumulator-style operators (one operand passed by pointer edited in place to be the return value, another read as a const, and the function returns void) for more speed if that helps. The only little improvement I made was implementing Karatsuba multiplication for complex numbers (a trick that comes up often in algorithmics is to limit the number of costly operation you need to do, like float multiplication, by replacing some of them by simpler operations, like two/three float additions). I might try to code a more optimized multiplication and exponentiation of complex numbers, for example using the polar form, if I need it in the future as well, but for now I'm good. 
[I think, given the usage of complex numbers especially in quantum mechanics, it might be worth to invent a chip that can do operations on complex numbers faster, if anyone is interested, there might be a market for that, especially if you make a cloud-based supercalculator from it for physicists.]

I'll let you check out my code in my libft for polynomials. I made real polynomials and complex polynomials, but more functions for the complex polynomials though, because they were the only ones that were useful here. The basic idea is to limit the degree of polynomials so you don't have to allocate anything: it's too costly to use polynomials that get too big anyways, unless you really need to. Evaluation of an input value is done through something called Hörner's algorithm (there are two different algorithms called Hörner's algorithm, just so you know), which is the best algorithm for that purpose.

Julia, Mandelbrot and Burningship dwell functions are rather straightforward to implement in the way I did, I believe. You can even come up with your own dwell functions for the heck of it with this system and see what happens. Make a name for yourself discovering new classes of fractals ! Send me some of your ideas for dwell functions if you'd like me to put them in my code, it might be fun !

If you want to implement Newton, the abstract, generalist approach to polynomials I took will be a lot more costly computationally, especially for a dynamic display (don't even try without implementing FFT first, which I personally still haven't taken the time to understand and implement), because Newton-dwell checks proximity of the point being iterated upon to the n roots of the polynomial, so you'll lose a lot of time for bigger polynomials (which I remind you have as many roots as their degree, so that can add up to a lot of operations). What I did was implement a simpler version of Newton. It's less cool, but it works well with my system and stil gives some rather pretty results. I might get back to trying the true Newton dwell function at some point when I understand threading or GPU programming a bit better.

I also added a custom protocol to show what was possible with a general ETF machine like this. It's a pretty weird protocol but it gives interesting results. It works by taking z_input, evaluating it into "P(z_input) = z1", then we find "z2 = (abs(z1.real), -z1.im)", then we take the average "z_output = (z1 + P(z2)) / 2".


**V) Optimization: the Mariani-Silver algorithm**

One of the major problems of trying to do math-y code in something else than a language that supports functional programming natively is that **there is always a trade-off between abstraction/generality and optimization/speciality**. I had already killed most optimization when I decided I wanted to be able to call any polynomial for my dwell functions. For this reason, there were many other places I needed to find ways to optimize. The most important of which is that a naive point-by-point rendering algorithm could not work, as it would be in O(h×w) where h is the height and w the width of the render space (the mlx window); and the operation called is our dwell function (which is very costly).

Instead, I opted for the Mariani-Silver algorithm, which is the fastest of the "very precise" rendering algorithms for fractals (you have faster ones which lose precision progressively). The idea is to divide the space you're studying into a self-similar tesselation. This sounds complicated, but look at the images here and you'll understand immediately: https://mrob.com/pub/muency/marianisilveralgorithm.html . Let's stick with the version with the squares, because this is what I implemented: it's the simplest to do.

It's the same principle as a quadtree. You have a function which divides your window into four quadrants, four squares. For every square, you calculate the dwell for each point at the boundary. You keep these values in a "dwell array" of size h×w (int dwellarr[WIN_H][WIN_W]). If every single point has the same dwell for a given figure, one considers it is a square containing a unique dwell on its inside as well, so you fill the values for the inside of the square with the same unique value as that of the dwell you found for the boundary. This saves you from calling your costly dwell function on all the points inside the square. If the boundary has at least two different dwells, you subdivide the square into 4 smaller squares recursively and apply the same protocol.

If our window is square, h×w = n², where n = w = h. This algorithm has a best time of O(n) and a worst time of O(n²) (like point-by-point), but the median time should be around O(n×lg(n)), if I'm not mistaken (which I might be, I've never been really good at complexity analysis for algorithms, never got enough practice). So it's still a pretty nice improvement, especially considering that the points that are filled in are often regions that go up to MAX_DWELL, so a lot of calls to Hörner's algorithm for complex polynomials (which I called eval_cpoly()) are removed.


**VI) Observations and conclusions**

Since rendering a fractal well requires a lot of optimization, and this is what is normally asked of you when you do Fract'ol, I really went against the current of what was required. Nevertheless, it was a very interesting exercise, trying to divide each step of escape-time fractal generation into a different, general, independent module (isolating dwell calculation, rendering and color), and having each module work coherently with the rest. All this while maintaining a certain standard of speed.

This went rather well considering the complexity and optimization loss of calling Hörner by default, even on polynomials with many of their coefficients equal to zero. Nevertheless, there's only so much you can do when you're looking for high-performance computing on an abstract framework and you're limited to a single thread and don't use the GPU (my bad, I didn't go THAT far (yet...)). For real-world purposes in HPC, you'd generally use a language like SciLab or MatLab, or a specialized framework like TensorFlow (and a TPU). 

Concerning color, once you have a dwell array that works well, it's rather easy to implement palettes independently from the rest. You build a palette for every possible dwell value, then just call the colors from this palette by calling the dwell in the index for every point in the dwell array, not much to it. Continuous/smooth color is a more complicated subject, you can look here: http://www.iquilezles.org/www/articles/mset_smooth/mset_smooth.htm . I might attempt it at some point.

Another fun thing I did easily was have the mouse hover effect that changes a fractal in real time just update a given coefficient of my polynomial: for this reason, I can have this effect for any dwell function, and any polynomial. I also implemented a setting which alters the index of the coef I'm updating in real time, to have a way to "navigate" through the whole of C[X] (space of polynomials over C) within the software. It runs fast enough to be viable for polynomials within our allowed range.

I also used a small trick to only call the render() function when the mouse moved a certain amount, or prevented the call to render() when the mouse was moving too fast.

Have a look at my git, you'll see in the /doc/ folder some links or pdfs which might help you if you were to attempt the same exercise I describe here (you should, it's a great way to learn your way around polynomials if you haven't yet). There are also some links to things I want to try in the future, but probably won't until I've done RT (you think I'm not crazy enough to attempt projections of fractals of dimension > 2 ? please: I can't wait. xd) 

https://github.com/Fulguritude/42_fractol


**VII) "What you said about math was really cool but... where could I learn this stuff, how could I learn this stuff...? Is it even for me ...?"**

Your brain is *literally* a mathematical machine. You know, a computer ? Oh, and you're a 42 student ? Yes, this is for you. Remember the entry test with the arrows that feels like a coding game ? That's mathematics. Even if you weren't a 42 student, mathematics (real, intuitive mathematics) is as natural to your brain as music is. Zelda ? A bag of items (a bag of theorems) to solve a puzzle in a given room (to solve a given problem/hypothesis) ? It's the same process.

If you're still scared of math because of your past (believe me, I can understand you; and after all, there's not much visuals here in this text, and visuals ARE NECESSARY), then look at it this way: **mathematics is like music, and the computer is the "instrument" of mathematics**. It's a lot easier to start solfege once you know an instrument well (=know how to program) and have listened to music (=have played and elaborated strategies for video games, strategy games, or puzzles) rather than before you're familiar with music in this way. **Without this intuitive cultural background, theory is ALWAYS incomprehensible.** Trust me, I didn't get this far without understanding that. By learning how to code, you've done the hardest part: learning an instrument to the point it becomes natural. But solfege can still open up a lot of doors, like composition (=being able to formalize your software and know where you're going before you start coding, to avoid wasting time on bad experiments, and have it be proven to be solid), better improvisation (=code better, more efficiently, and more weirdly, because you know mathematical tricks and licks), and discovering whole new genres of music that you didn't even know you might like (=realizing how what you like in computer science relates to other fields of mathematics and science, and how you might apply some crazy math to make truly beautiful and useful code).

So where should you *start* ?

First, I'd advise you watch this series of 5 videos: https://www.youtube.com/watch?v=nkvVR-sKJT8
History is incredibly important to understand *why* scientific fields (even mathematics) are structured the way they are. After that, watch this, it's not perfect, but it's pretty good for such a hard exercise as making a 2D map of mathematics and it'll give you a good idea of what's present: https://www.youtube.com/watch?v=OmJ-4B-mS-Y
With this, you should be able to place the new things you learn in your mind a bit more clearly and with better organization.

Also, GDC has lots of game developers who use video games to create and share beautiful math:
https://www.youtube.com/watch?v=WumyfLEa6bU
https://www.youtube.com/watch?v=tR-9oXiytsk
https://www.youtube.com/watch?v=ed2zmmcEryw

Then you should subscribe to the following YouTube channels: 3blue1brown (watch ALL of these), LeiosOS, Think Twice, Welch Labs, Vihart, PBS Infinite Series, Mathologer, Numberphile, Computerphile, and while we're at it, VSauce, Veritasium, Kurzgesagt, minutephysics, and Looking Glass Universe. That's already pretty good for now.

Read Quanta Magazine regularly: mathematics is in all the sciences, and you should stay up to date, research is going very, very, very fast; and research is *accelerating, becoming even faster, just as intensely as fast as it is already going*... This idea, which is the fundamental nature of the exponential function, is something our linear minds have deep trouble understanding; and yet technological innovation is on an exponential curve. So it's better to overestimate than underestimate this phenomenon.

If you now want to master the techniques and the practice of mathematics, you always have Khan Academy, it'll get you up to roughly the third year of university while REALLY holding your hand (a bit too much when math starts to become easier to 'speak' for you; that's when you should move on to university textbooks).

Once you've done that, you have blogs like http://www.math3ma.com/ or https://infinityplusonemath.wordpress.com/ which will explain advanced concepts like the Yoneda perspective or theories like category theory (if you want to know how to use functional programming at a high level, you'll need this) or general relativity in a VERY pedagogical way.

And for all the rest, there's always someone who has asked Quora "What's a good introductory textbook to [insert field of math or CS or physics or biology here] ?", so if you google "quora introductory textbook [scientific field]" you'll generally get some good results.

At that point, knowing Springer sells its textbooks at ~250$ a pop, and you won't know beforehand if the textbook works for you (if it's pedagogical, if it's approachable at your level) I'm a full on advocate for piracy until the system can find a better way to compensate people for their work, rather than pay abusive dividends to Disney, Universal, Springer and all these parasites trying to make it seem like data can't be copied with a single click. The Internet and the PDF are greater than the most utopian fantasies of the librarians of Alexandria, and it is **CRIMINAL** not to let people use it freely. I owe too much to piracy (and want to give back to my society for this reason as well) to defend anything else. Give me a call if you need some links for pirating textbooks (I would put my go-to link here, but I don't want to get in trouble or get the school in trouble). 

[Seriously, how hard could it be to have a State-run streaming library for science texts and artistic media, where the tax exempt (like children and the jobless) could still have free access ? Richer or heavier users would pay a bit more as taxes, and creators would be compensated automatically with no middleman. You pay for schools even if you don't have kids, and this sort of universal access to knowledge is EVEN MORE important than schools !]

Remember: if something feels complicated, if you read, and re-read, and re-re-read the same passage without understanding why you can't seem to concentrate, in my experience, 99% of the time there's a word you THINK you know, but actually don't know. An "(algebraic) field" in mathematics is not a "(vector) field" in physics and it's not a "(plant) field" in agriculture and it's not a "(research) field" of science. **If you catch yourself rereading the same passage over and over again, stop. Ask yourself, word by word, "Could I define this, both formally and intuitively, in this context ?", and you'll generally find the cause of the "reading illusion" that made your mind glitch. Go back to the definition. Understand it, review it, work some examples and counter-examples, and you'll see the "reading illusion" and the complexity of what you were reading dissolve completely on their own.**

I can guarantee you: I learned most of my mathematics on my own this way, and it was a LOT easier than learning it in school, with this technique of "keeping in mind if I can define". When you get to go slow and digest everything, it is much easier to learn an "ecosystem of knowledge" like a scientific field, than it is to follow a teacher who doesn't even make sure you have every prerequisite, in school.


**VIII) Some philosophy of mathematics**

Finally, I'd like to share some very important ideas I've learned concerning the philosophy of mathematics over the years, some things I think would have helped me a lot had I known them earlier.


The first, which did not come first historically, but I consider most important, the idea of the "bestiary" of mathematics, the idea of "algebraic structures".

Towards the end of the 19th century, a few mathematicians were starting to think less and less of just algorithms like finding the roots of a polynomial, or integrating a function, and more and more about the mathematical structures in which polynomials or functions existed. The most famous of the solutions to approach this was Georg Cantor's "Set theory", which allowed most of mathematics to find a common basis for the 20th century. But at the same period, a woman, Emmy Noether (whom I consider personally to be the greatest mathematician of all time, although it is of course a question of personal taste), had an even better idea. **Her idea was that it wasn't JUST sets, it was sets that *behaved* a certain way, sets that had certain *properties*.** She developed along with Hilbert, Artin, and others, the theory of algebraic structures, and with it paved the way for the most explosive phase of mathematics research in history. [She also proved that mathematical invariants in a system, which we refer to as 'symmetries' in a more general sense, meant physical invariants in physical systems; this idea is the backbone of both general relativity and quantum mechanics...]

Learning this vocabulary/bestiary of "sets with properties" (set, group, ring, field, vector space, algebra, category, topology, measurable space, etc) is what made me fall in love with mathematics. It allowed me to understand what it exactly was that I was using, why I was manipulating it in this way, and how I could build new things with these tools and see how they related, even if they seemed so different at first.

The main inheritor of Noether's ideas is category theory. To this day, it is the tool we use to teach PhD students in mathematics how to actually explore mathematical spaces and their relations. However, there has been recently some extremely interesting work to follow up on category theory by Vladimir Voevodsky (RIP... ='( ). His Homotopy-Type Theory, which already has applications in using functional programming to make mathematical proofs (yes you read that correctly, automating math research), might be within this century the key to build a single common framework for the whole of mathematics and computer science. A "Set theory" for the early IIIrd millenium.


The second idea I want to share is the Yoneda perspective, which is the second core idea of category theory. 

A "category", intuitively, is the structure which contains all the algebraic structures of a certain type (for example, VectR is the category of all vector spaces with R as a scalar field, and Fld is the category of all algebraic fields). Category theory studies the way spaces exist in relations to one another, and how categories exist in relation to one another. Category theory is extremely abstract and beautiful, because it shows that the whole of mathematics can be understood in terms of functions; that every mathematical object can in a sense be understood as being an abstract "input-output" of some sort, even things that seem fixed like numbers or vectors. It is an *extremely powerful* way of seeing the world, because it is so abstract, concise, and ecosystemic.

**The Yoneda perspective is the idea that once you understand everything as input-output, mathematical objects can be ENTIRELY DEFINED AND UNDERSTOOD by the relationship they have with the rest of mathematical objects.** This gives you tools to probe mathematical structures and their behavior. You don't understand fractals ? Ask yourself: what does the space in which fractals exist look like ? This is literally the method we're using in physics to figure out which new theories we can invent https://www.quantamagazine.org/there-are-no-laws-of-physics-theres-only-the-landscape-20180604/ . Or ask yourself: how do I go from one space of fractals to another space of fractals, while maintaining core properties of fractals ?

A very powerful idea in this context is that of the isomorphism, which we used briefly earlier. A morphism is a transformation that is stable within a category, a transformation that "maintains the properties of a set considered to be an algebraic structure". For example, your matrices with coefficients in R are the morphisms of VectR, because they take as input a 'vector space over R' and also return a 'vector space over R' (formally, maintaining the structure is basically keeping the order of the operators unaffected, whether you calculate their result before or after the morphism). An isomorphism is a morphism that keeps "everything" about the structure: it is the tool we use to prove that even if we have different languages to describe different parts of mathematics, two structures are actually one and the same within a given category. It is the tool we use to prove that two things are synonymous within mathematics. For example, the '=' sign, or logical equivalence '<=>' can both be considered isomorphisms within the appropriate context.

[By the way, Cat is the category in which all categories exist, and the morphisms in Cat are called "functors", in case you've ever heard of them while trying to wrap your head around some complex protocol of functional programming. Functors allow you to probe how categories relate. They also allow you to create new algebraic structures from old algebraic structures, and allow you to know if you're going somewhere knew, or just repeating something we've already done. Case in point, it is because Voevodsky found an "isofunctor" between Homotopy Theory and Type Theory that we might one day soon enough get a single common theoretical framework for both mathematics and computer science.]


The third idea is that of axiomatization.

True mathematics starts with Euclid, because Euclid invented rigor: the idea that you should start from something true and use that to figure out which other things are true. But Euclid realized that you have to start somewhere, with some idea you accept as true, even if you can't *prove* it to be true ('principles', 'axioms', 'postulates': all of these are synonyms for this idea). He was deeply aware of, and deeply troubled, by this idea. Yet this seemed the only possible condition to build a formal system (and thus to build formal, solid mathematics): axiomatization.

Fast-forward ~22 centuries, in the year 1900. David Hilbert, most famous mathematician of his time, is invited for a great congress of mathematicians at the Sorbonne in Paris. At a conference he holds, he presents 23 unsolved problems which will define the 20th century in mathematics. Some are still unsolved (and #8, the Riemann hypothesis, is literally worth a million dollars). But the most interesting one, in my opinion, is #2, commonly referred to as "Hilbert's program": to show that the axioms of arithmetic (the math of the integers) are well-founded and lead to a coherent system, upon which the rest of mathematics can rely.

In 1931, a young Austrian logician by the name of Kurt Gödel showed with two theorems that it was impossible to prove the coherence of arithmetic using arithmetic as a formal system. That all knowledge, even mathematical knowledge, would have to rely on axioms, on things which cannot be proven and in which you must have faith. And from the ashes of Hilbert's program, this extremely humbling moment of humanity's intellect, this "okay, so there are things we CAN'T know, even in math...", rose a new way of thinking, a "so what CAN we do with math ?". From this way of thinking the theory of computability was born, and with it, our technological age. [Turing was deeply influenced by the work of Gödel in particular; his solution to the Halting problem is basically the Incompleteness theorems applied to computability. Another important idea I could have talked about is how **proof and computation are actually two sides of the same coin**, two ways of seeing the same universal principle. If you get into Gödel, Turing, and Voevodsky's work, Type Theory, functional programming and the Coq framework, you'll get the idea. It will still be very important this century, as we automate scientific research more and more.]

(If you're interested in this third idea, I suggest you read the excellent comic book LogiComix which narrates in detail how philosophers and mathematicians merged towards the end of the 19th century to give birth to the field of logic, and with it, how we tricked rocks into becoming machines that think for us. You might also be interested by Douglas Hofstadter's *Gödel, Escher, Bach*, a book which uses the self-reference used in Gödel's proof to explore self-reference in human consciousness, and is a monument to literature, science and philosophy all by itself.)

[On a historical side note, Hindu mathematician have always been ahead of Europe when it came to techniques. Lots of things that hold the name of a European/Mediterranean scientist like Fourier were already discovered a century before in India. However, the Indians never really killed astrology and religion inside their own mathematics like Euclid and Socrates allowed us when laying the groundwork for Mediterranean science. This sacrality/ethereality of mathematics for Hindu mathematicians became a problem in sharing it worldwide and calling into question the things they took for granted, which might explain their loss of pre-eminence. That, and colonization, of course.]


The fourth idea is the idea of mathematics as a language.

You might have gotten a glimpse of it here, about what I meant by "formal", but this is something akin to describing how a Japanese person reads kanji to someone who does not understand how kanji work. How you compose them to build other meanings; how etymology and symbolism play a large role in the richness of the language. A haiku can only be read in Japanese, because a haiku in Japanese is not only a poem that uses sound and concise imagery, but is also a tiny rebus filled analogies, references and hidden meanings, compacted into 5-7-5 syllables, because of the vast amount of homophones, homographs, or compound words in Japanese overall.

The same could be said of music, on how one improvizes jazz or has a certain 'cultural upbringing' of sound. Victor Wooten will express it infinitely better than I will: https://www.youtube.com/watch?v=2zvjW9arAZ0

The power of mathematics as a language (or even, if you consider code and various formalisms, multiple languages) is that it is a language which we can construct and improve upon in order to remove human bias from human thinking. Only in a mathematical language can you express things like vector space of dimension 4 and over, in a way our minds which are limited to 3 spatial dimensions, 1 temporal dimension, and maybe a few color dimensions, could understand. Mathematical language provides a great standard against which to pit your ideas to see if they hold. It is a great way of "having an internal monologue" built for solving problems, like when you are coding or designing a solution to a problem with resources. It is a great way of making sense of your ideas on paper as well. It is a way of expressing the world in a testable way, and where you can most often trust the test and its results.


The fifth and final idea is the aesthetics of mathematics.

What makes a proof beautiful, a geometric figure beautiful ? Scratch that, what makes ANYTHING beautiful ?

The answer I learned from studying mathematics is this: **beauty is something as simple as possible that your mind can perceive, that also hides extreme richness and complexity which you can also perceive. "Taste" is the capacity to extract and appreciate complexity from simplicity.** This applies to a good proof, but also to a beautiful work of art, or a great story, or a great meal, or a great perfume, or a great song; a great ecstasy, or a great pain... Beauty is the sentiment your mind gets when part of its intelligence (whether it's sensory intelligence, emotional intelligence or rational intelligence) can "see everything clearly", because something extremely complex is eased, through abstraction, into something simple and easy to grapple with, but which retains all the richness it had when it was complex. Like holding a tree in your hand by the trunk, rather than grappling with a ile of leaves that fall all over the place. This is why Euler's identity is so beautiful, in particular: https://medium.com/q-e-d/7-times-8-is-54-the-call-for-math-memorization-950abcfc1fc7 . Euler's identity (and the linked Euler's formula) encodes the whole of trigonometry within a single line. Indeed, most (if not 'all'?) of art is a subtle play between order/symmetry/simplicity and chaos/strangeness/complexity. I think this is fundamentally because our brain, above all, is a mathematical machine. It loves excelling at complex feats of intellect, action or perception, and it loves having an easy time doing so - with the problem still being a challenge and an exploration. It loves building the complex with ease out of the abstract.

*For this reason, mathematics, being the ultimate quest for abstraction, is also the ultimate quest for beauty.*


**IX) Post-scriptum**

I'm also currently writing another text to explain mathematics, epistemology and science to anyone who has roughly a high school level in math and computer science. It's in French, it's not done, it's imperfect, it lacks nuance in some philosophical parts, but you'll get to learn many of the tools I described here more in depth if you're interested; most notably all things related to set theory, logic, and linear algebra, which are the sinequanone entry points into higher mathematics and computer science. All of this while keeping everything centered around algebraic structures, so the bestiary feels vivid, connected, and understandable. I'm looking to expand it with computer science, more math, and other sciences in the future as well. Just hit me up, give me an email, and I'll send you the current pdf if you're interested.

If anyone knows LaTeX and could give me a small rundown of the language so I can convert my text easily and put it on a github where it could be public and collaborative, I'm all ears. :) If not, I'll learn it on my own when I get the time.

And with that, I wish you the best of days !
